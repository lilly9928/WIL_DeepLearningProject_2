{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image>An image of two cats.<|endofchunk|><image>An image of a bathroom sink.<|endofchunk|><image>An image of a buffet table.<|endofchunk|>\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Step 1: Load images\n",
    "\"\"\"\n",
    "demo_image_one = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "demo_image_two = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028137.jpg\",\n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "query_image = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028352.jpg\", \n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 2: Preprocessing images\n",
    "Details: For OpenFlamingo, we expect the image to be a torch tensor of shape \n",
    " batch_size x num_media x num_frames x channels x height x width. \n",
    " In this case batch_size = 1, num_media = 3, num_frames = 1,\n",
    " channels = 3, height = 224, width = 224.\n",
    "\"\"\"\n",
    "vision_x = [image_processor(demo_image_one).unsqueeze(0), image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]\n",
    "vision_x = torch.cat(vision_x, dim=0)\n",
    "vision_x = vision_x.unsqueeze(1).unsqueeze(0)\n",
    "\n",
    "\"\"\"\n",
    "Step 3: Preprocessing text\n",
    "Details: In the text we expect an <image> special token to indicate where an image is.\n",
    " We also expect an <|endofchunk|> special token to indicate the end of the text \n",
    " portion associated with an image.\n",
    "\"\"\"\n",
    "tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "lang_x = tokenizer(\n",
    "    [\"<image>An image of two cats.<|endofchunk|><image>An image of a bathroom sink.<|endofchunk|><image>An image of\"],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 4: Generate text\n",
    "\"\"\"\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x,\n",
    "    lang_x=lang_x[\"input_ids\"],\n",
    "    attention_mask=lang_x[\"attention_mask\"],\n",
    "    max_new_tokens=20,\n",
    "    num_beams=3,\n",
    ")\n",
    "\n",
    "print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  \n",
      "     Think step by step to answer the question.\n",
      "     <image>\n",
      "     Question: Is the cat above a mat?\n",
      "    Program:\n",
      "    BOX0=LOC(image=IMAGE,object='cat')\n",
      "    IMAGE0=CROP_BELOW(image=IMAGE,box=BOX0)\n",
      "    BOX1=LOC(image=IMAGE0,object='mat')\n",
      "    ANSWER0=COUNT(box=BOX1)\n",
      "    ANSWER1=EVAL(expr=\"'yes' if {ANSWER0} > 0 and else 'no'\")\n",
      "    FINAL_RESULT=RESULT(var=ANSWER1)\n",
      "    <|endofchunk|>\n",
      "        <image>Question: Are there trains or fences in this scene?\n",
      "    Program:\n",
      "    BOX0=LOC(image=IMAGE,object='train')\n",
      "    BOX1=LOC(image=IMAGE,object='fence')\n",
      "    ANSWER0=COUNT(box=BOX0)\n",
      "    ANSWER1=COUNT(box=BOX1)\n",
      "    ANSWER2=EVAL(expr=\"'yes' if {ANSWER0} + {ANSWER1} > 0 else 'no'\")\n",
      "    FINAL_RESULT=RESULT(var=ANSWER)\n",
      "    <image>Question: Which place is it?\n",
      "    Program:\n",
      "    BOX0=LOC(image=IMAGE,object='place')\n",
      "    BOX1=LOC(image=IMAGE,object='house')\n",
      "    ANSWER0=COUNT(box=BOX0)\n",
      "    ANSWER1=COUNT(box=BOX1)\n",
      "    ANSWER2=EVAL(expr=\"'yes' if {ANSWER0} + {ANSWER1} > 0 else 'no'\")\n",
      "    FIN\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Step 1: Load images\n",
    "\"\"\"\n",
    "demo_image_one = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "demo_image_two = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028137.jpg\",\n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "query_image = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028352.jpg\", \n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 2: Preprocessing images\n",
    "Details: For OpenFlamingo, we expect the image to be a torch tensor of shape \n",
    " batch_size x num_media x num_frames x channels x height x width. \n",
    " In this case batch_size = 1, num_media = 3, num_frames = 1,\n",
    " channels = 3, height = 224, width = 224.\n",
    "\"\"\"\n",
    "vision_x = [image_processor(demo_image_one).unsqueeze(0), image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]\n",
    "vision_x = torch.cat(vision_x, dim=0)\n",
    "vision_x = vision_x.unsqueeze(1).unsqueeze(0)\n",
    "\n",
    "\"\"\"\n",
    "Step 3: Preprocessing text\n",
    "Details: In the text we expect an <image> special token to indicate where an image is.\n",
    " We also expect an <|endofchunk|> special token to indicate the end of the text \n",
    " portion associated with an image.\n",
    "\"\"\"\n",
    "tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "lang_x = tokenizer(\n",
    "    [\"\"\"\n",
    "     Think step by step to answer the question.\n",
    "     <image>\n",
    "     Question: Is the cat above a mat?\n",
    "    Program:\n",
    "    BOX0=LOC(image=IMAGE,object='cat')\n",
    "    IMAGE0=CROP_BELOW(image=IMAGE,box=BOX0)\n",
    "    BOX1=LOC(image=IMAGE0,object='mat')\n",
    "    ANSWER0=COUNT(box=BOX1)\n",
    "    ANSWER1=EVAL(expr=\"'yes' if {ANSWER0} > 0 and else 'no'\")\n",
    "    FINAL_RESULT=RESULT(var=ANSWER1)\n",
    "    <|endofchunk|>\n",
    "        <image>Question: Are there trains or fences in this scene?\n",
    "    Program:\n",
    "    BOX0=LOC(image=IMAGE,object='train')\n",
    "    BOX1=LOC(image=IMAGE,object='fence')\n",
    "    ANSWER0=COUNT(box=BOX0)\n",
    "    ANSWER1=COUNT(box=BOX1)\n",
    "    ANSWER2=EVAL(expr=\"'yes' if {ANSWER0} + {ANSWER1} > 0 else 'no'\")\n",
    "    FINAL_RESULT=RESULT(var=ANSWER)\n",
    "    <image>Question: Which place is it?\"\"\"],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 4: Generate text\n",
    "\"\"\"\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x,\n",
    "    lang_x=lang_x[\"input_ids\"],\n",
    "    attention_mask=lang_x[\"attention_mask\"],\n",
    "    max_new_tokens=100,\n",
    "    num_beams=3,\n",
    ")\n",
    "\n",
    "print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  \n",
      "     Think step by step to answer the question.\n",
      "     <image>\n",
      "     Question: Is the cat above a mat?\n",
      "    Program:\n",
      "    BOX0=LOC(image=IMAGE,object='cat')\n",
      "    IMAGE0=CROP_BELOW(image=IMAGE,box=BOX0)\n",
      "    BOX1=LOC(image=IMAGE0,object='mat')\n",
      "    ANSWER0=COUNT(box=BOX1)\n",
      "    ANSWER1=EVAL(expr=\"'yes' if {ANSWER0} > 0 and else 'no'\")\n",
      "    FINAL_RESULT=RESULT(var=ANSWER1)\n",
      "    <|endofchunk|>\n",
      "        <image>Question: Where is the towel?\n",
      "    Program:\n",
      "    BOX0=LOC(image=IMAGE,object='towel')\n",
      "    ANSWER0=VQA(image=IMAGE,question='What is near the towel?')\n",
      "    BOX1=LOC(image=IMAGE,object='{ANSWER0}')\n",
      "    ANSWER2=VQA(image={BOX1},question='What is this?')\n",
      "    FINAL_RESULT=RESULT(var=ANSWER2)\n",
      "    <image>Question: What is this?\n",
      "    Program:\n",
      "    BOX0=LOC(image=IMAGE,object='{ANSWER0}')\n",
      "    ANSWER0=VQA(image=IMAGE,question='What is this?')\n",
      "    BOX1=LOC(image=IMAGE,object='{ANSWER1}')\n",
      "    ANSWER2=VQA(image=IMAGE,question='What is this?')\n",
      "    FINAL_RESULT=RESULT(var=ANSWER\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Step 1: Load images\n",
    "\"\"\"\n",
    "demo_image_one = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "demo_image_two = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028137.jpg\",\n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "query_image = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028352.jpg\", \n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 2: Preprocessing images\n",
    "Details: For OpenFlamingo, we expect the image to be a torch tensor of shape \n",
    " batch_size x num_media x num_frames x channels x height x width. \n",
    " In this case batch_size = 1, num_media = 3, num_frames = 1,\n",
    " channels = 3, height = 224, width = 224.\n",
    "\"\"\"\n",
    "vision_x = [image_processor(demo_image_one).unsqueeze(0), image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]\n",
    "vision_x = torch.cat(vision_x, dim=0)\n",
    "vision_x = vision_x.unsqueeze(1).unsqueeze(0)\n",
    "\n",
    "\"\"\"\n",
    "Step 3: Preprocessing text\n",
    "Details: In the text we expect an <image> special token to indicate where an image is.\n",
    " We also expect an <|endofchunk|> special token to indicate the end of the text \n",
    " portion associated with an image.\n",
    "\"\"\"\n",
    "tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "lang_x = tokenizer(\n",
    "    [\"\"\"\n",
    "     Think step by step to answer the question.\n",
    "     <image>\n",
    "     Question: Is the cat above a mat?\n",
    "    Program:\n",
    "    BOX0=LOC(image=IMAGE,object='cat')\n",
    "    IMAGE0=CROP_BELOW(image=IMAGE,box=BOX0)\n",
    "    BOX1=LOC(image=IMAGE0,object='mat')\n",
    "    ANSWER0=COUNT(box=BOX1)\n",
    "    ANSWER1=EVAL(expr=\"'yes' if {ANSWER0} > 0 and else 'no'\")\n",
    "    FINAL_RESULT=RESULT(var=ANSWER1)\n",
    "    <|endofchunk|>\n",
    "        <image>Question: Where is the towel?\n",
    "    Program:\n",
    "    BOX0=LOC(image=IMAGE,object='towel')\n",
    "    ANSWER0=VQA(image=IMAGE,question='What is near the towel?')\n",
    "    BOX1=LOC(image=IMAGE,object='{ANSWER0}')\n",
    "    ANSWER2=VQA(image={BOX1},question='What is this?')\n",
    "    FINAL_RESULT=RESULT(var=ANSWER2)\n",
    "    <image>Question: What is this?\"\"\"],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 4: Generate text\n",
    "\"\"\"\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x,\n",
    "    lang_x=lang_x[\"input_ids\"],\n",
    "    attention_mask=lang_x[\"attention_mask\"],\n",
    "    max_new_tokens=100,\n",
    "    num_beams=3,\n",
    ")\n",
    "\n",
    "print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "GQA_CURATED_EXAMPLES=[\n",
    "\"\"\"\n",
    "Think step by step to answer the question.\n",
    "<image>\n",
    "Question: Is the vehicle in the top of the image?\n",
    "Program:\n",
    "BOX0=LOC(image=IMAGE,object='TOP')\n",
    "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
    "BOX1=LOC(image=IMAGE0,object='vehicle')\n",
    "ANSWER0=COUNT(box=BOX1)\n",
    "ANSWER1=EVAL(expr=\"'yes' if {ANSWER0} > 0 else 'no'\")\n",
    "FINAL_RESULT=RESULT(var=ANSWER1)\n",
    "<|endofchunk|>\n",
    "<image>\n",
    "Question: Who is carrying the umbrella?\n",
    "Program:\n",
    "BOX0=LOC(image=IMAGE,object='umbrella')\n",
    "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
    "ANSWER0=VQA(image=IMAGE0,question='Who is carrying the umbrella?')\n",
    "FINAL_RESULT=RESULT(var=ANSWER0)\n",
    "<|endofchunk|>\n",
    "<image>\n",
    "Question: Which place is it?\n",
    "Program:\n",
    "ANSWER0=VQA(image=IMAGE,question='Which place is it?')\n",
    "FINAL_RESULT=RESULT(var=ANSWER0)\n",
    "<|endofchunk|>\n",
    "<image>\n",
    "Question: Is the pillow in the top part or in the bottom of the picture?\n",
    "Program:\n",
    "BOX0=LOC(image=IMAGE,object='TOP')\n",
    "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
    "BOX1=LOC(image=IMAGE0,object='pillow')\n",
    "ANSWER0=COUNT(box=BOX1)\n",
    "ANSWER1=EVAL(expr=\"'top' if {ANSWER0} > 0 else 'bottom'\")\n",
    "FINAL_RESULT=RESULT(var=ANSWER1)\n",
    "<|endofchunk|>\n",
    "<image>\n",
    "Question: Which side is the food on?\n",
    "Program:\n",
    "BOX0=LOC(image=IMAGE,object='RIGHT')\n",
    "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
    "BOX1=LOC(image=IMAGE0,object='food')\n",
    "ANSWER0=COUNT(box=BOX1)\n",
    "ANSWER1=EVAL(expr=\"'right' if {ANSWER0} > 0 else 'left'\")\n",
    "FINAL_RESULT=RESULT(var=ANSWER1)\n",
    "<|endofchunk|>\n",
    "<image>\n",
    "Question: Do the post and the sign have a different colors?\n",
    "Program:\n",
    "BOX0=LOC(image=IMAGE,object='post')\n",
    "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
    "BOX1=LOC(image=IMAGE,object='sign')\n",
    "IMAGE1=CROP(image=IMAGE,box=BOX1)\n",
    "ANSWER0=VQA(image=IMAGE0,question='What color is the post?')\n",
    "ANSWER1=VQA(image=IMAGE1,question='What color is the sign?')\n",
    "ANSWER2=EVAL(expr=\"'yes' if {ANSWER0} != {ANSWER1} else 'no'\")\n",
    "FINAL_RESULT=RESULT(var=ANSWER2)\n",
    "<|endofchunk|>\n",
    "<image>\n",
    "Question: Does the traffic cone have white color?\n",
    "Program:\n",
    "BOX0=LOC(image=IMAGE,object='traffic cone')\n",
    "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
    "ANSWER0=VQA(image=IMAGE0,question='What color is the traffic cone?')\n",
    "ANSWER1=EVAL(expr=\"'yes' if {ANSWER0} == 'white' else 'no'\")\n",
    "FINAL_RESULT=RESULT(var=ANSWER1)\n",
    "<|endofchunk|>\n",
    "<image>\n",
    "Question: Are these animals of different species?\n",
    "Program:\n",
    "ANSWER0=VQA(image=IMAGE,question='Are these animals of different species?')\n",
    "FINAL_RESULT=RESULT(var=ANSWER0)\n",
    "<|endofchunk|>\n",
    "<image>\n",
    "Question: Which side of the image is the chair on?\n",
    "Program:\n",
    "BOX0=LOC(image=IMAGE,object='RIGHT')\n",
    "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
    "BOX1=LOC(image=IMAGE0,object='chair')\n",
    "ANSWER0=COUNT(box=BOX1)\n",
    "ANSWER1=EVAL(expr=\"'right' if {ANSWER0} > 0 else 'left'\")\n",
    "FINAL_RESULT=RESULT(var=ANSWER1)\n",
    "<|endofchunk|>\n",
    " <image>Question: What is this?\n",
    " Program:\n",
    "\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  \n",
      "Think step by step to answer the question.\n",
      "<image>\n",
      "Question: Is the vehicle in the top of the image?\n",
      "Program:\n",
      "BOX0=LOC(image=IMAGE,object='TOP')\n",
      "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
      "BOX1=LOC(image=IMAGE0,object='vehicle')\n",
      "ANSWER0=COUNT(box=BOX1)\n",
      "ANSWER1=EVAL(expr=\"'yes' if {ANSWER0} > 0 else 'no'\")\n",
      "FINAL_RESULT=RESULT(var=ANSWER1)\n",
      "<|endofchunk|>\n",
      "<image>\n",
      "Question: Who is carrying the umbrella?\n",
      "Program:\n",
      "BOX0=LOC(image=IMAGE,object='umbrella')\n",
      "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
      "ANSWER0=VQA(image=IMAGE0,question='Who is carrying the umbrella?')\n",
      "FINAL_RESULT=RESULT(var=ANSWER0)\n",
      "<|endofchunk|>\n",
      "<image>\n",
      "Question: Which place is it?\n",
      "Program:\n",
      "ANSWER0=VQA(image=IMAGE,question='Which place is it?')\n",
      "FINAL_RESULT=RESULT(var=ANSWER0)\n",
      "<|endofchunk|>\n",
      "<image>\n",
      "Question: Is the pillow in the top part or in the bottom of the picture?\n",
      "Program:\n",
      "BOX0=LOC(image=IMAGE,object='TOP')\n",
      "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
      "BOX1=LOC(image=IMAGE0,object='pillow')\n",
      "ANSWER0=COUNT(box=BOX1)\n",
      "ANSWER1=EVAL(expr=\"'top' if {ANSWER0} > 0 else 'bottom'\")\n",
      "FINAL_RESULT=RESULT(var=ANSWER1)\n",
      "<|endofchunk|>\n",
      "<image>\n",
      "Question: Which side is the food on?\n",
      "Program:\n",
      "BOX0=LOC(image=IMAGE,object='RIGHT')\n",
      "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
      "BOX1=LOC(image=IMAGE0,object='food')\n",
      "ANSWER0=COUNT(box=BOX1)\n",
      "ANSWER1=EVAL(expr=\"'right' if {ANSWER0} > 0 else 'left'\")\n",
      "FINAL_RESULT=RESULT(var=ANSWER1)\n",
      "<|endofchunk|>\n",
      "<image>\n",
      "Question: Do the post and the sign have a different colors?\n",
      "Program:\n",
      "BOX0=LOC(image=IMAGE,object='post')\n",
      "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
      "BOX1=LOC(image=IMAGE,object='sign')\n",
      "IMAGE1=CROP(image=IMAGE,box=BOX1)\n",
      "ANSWER0=VQA(image=IMAGE0,question='What color is the post?')\n",
      "ANSWER1=VQA(image=IMAGE1,question='What color is the sign?')\n",
      "ANSWER2=EVAL(expr=\"'yes' if {ANSWER0}!= {ANSWER1} else 'no'\")\n",
      "FINAL_RESULT=RESULT(var=ANSWER2)\n",
      "<|endofchunk|>\n",
      "<image>\n",
      "Question: Does the traffic cone have white color?\n",
      "Program:\n",
      "BOX0=LOC(image=IMAGE,object='traffic cone')\n",
      "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
      "ANSWER0=VQA(image=IMAGE0,question='What color is the traffic cone?')\n",
      "ANSWER1=EVAL(expr=\"'yes' if {ANSWER0} == 'white' else 'no'\")\n",
      "FINAL_RESULT=RESULT(var=ANSWER1)\n",
      "<|endofchunk|>\n",
      "<image>\n",
      "Question: Are these animals of different species?\n",
      "Program:\n",
      "ANSWER0=VQA(image=IMAGE,question='Are these animals of different species?')\n",
      "FINAL_RESULT=RESULT(var=ANSWER0)\n",
      "<|endofchunk|>\n",
      "<image>\n",
      "Question: Which side of the image is the chair on?\n",
      "Program:\n",
      "BOX0=LOC(image=IMAGE,object='RIGHT')\n",
      "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
      "BOX1=LOC(image=IMAGE0,object='chair')\n",
      "ANSWER0=COUNT(box=BOX1)\n",
      "ANSWER1=EVAL(expr=\"'right' if {ANSWER0} > 0 else 'left'\")\n",
      "FINAL_RESULT=RESULT(var=ANSWER1)\n",
      "<|endofchunk|>\n",
      " <image>Question: What is this?\n",
      " Program:\n",
      " ANSWER0=VQA(image=IMAGE,question='What is this?')\n",
      " FINAL_RESULT=RESULT(var=ANSWER0)\n",
      "<|endofchunk|>\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "from functools import partial\n",
    "\n",
    "# sys.path.append('/home/user2/code/WIL_DeepLearningProject_2/VisualProgramming')\n",
    "\n",
    "# from prompt.gqa import create_prompt\n",
    "\n",
    "image_id = ['n437038', 'n153818', 'n542565', 'n428090', 'n435687', 'n167620', 'n70342', 'n418470', 'n77875']\n",
    "\n",
    "vision_x = []\n",
    "\n",
    "image_path = '/data2/NS/GQA/images/images/'\n",
    "for i in image_id:\n",
    "    demo_image = Image.open(os.path.join(image_path,i+\".jpg\")) \n",
    "    vision_x.append(image_processor(demo_image).unsqueeze(0))\n",
    "    \n",
    "query_image = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028352.jpg\", \n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "vision_x.append(image_processor(query_image).unsqueeze(0))\n",
    "\n",
    "vision_x = torch.cat(vision_x, dim=0)\n",
    "vision_x = vision_x.unsqueeze(1).unsqueeze(0)\n",
    "question = \"What is this?\"\n",
    "\n",
    "tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "lang_x = tokenizer(\n",
    "   GQA_CURATED_EXAMPLES,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x,\n",
    "    lang_x=lang_x[\"input_ids\"],\n",
    "    attention_mask=lang_x[\"attention_mask\"],\n",
    "    max_new_tokens=100,\n",
    "    num_beams=3,\n",
    ")\n",
    "\n",
    "print(\"Generated text: \", tokenizer.decode(generated_text[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KIJI",
   "language": "python",
   "name": "kiji"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d4ce9b068c66b067ddc9a0e35241e261eb403701c001fd5e69fb192b3569b41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
