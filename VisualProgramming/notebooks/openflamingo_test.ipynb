{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 36\u001b[0m\n\u001b[1;32m     21\u001b[0m query_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\n\u001b[1;32m     22\u001b[0m     requests\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://images.cocodataset.org/test-stuff2017/000000028352.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     24\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     )\u001b[38;5;241m.\u001b[39mraw\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mStep 2: Preprocessing images\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03mDetails: For OpenFlamingo, we expect the image to be a torch tensor of shape \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m channels = 3, height = 224, width = 224.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m vision_x \u001b[38;5;241m=\u001b[39m [\u001b[43mimage_processor\u001b[49m(demo_image_one)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), image_processor(demo_image_two)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), image_processor(query_image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)]\n\u001b[1;32m     37\u001b[0m vision_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(vision_x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     38\u001b[0m vision_x \u001b[38;5;241m=\u001b[39m vision_x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_processor' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Step 1: Load images\n",
    "\"\"\"\n",
    "demo_image_one = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "demo_image_two = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028137.jpg\",\n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "query_image = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028352.jpg\", \n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 2: Preprocessing images\n",
    "Details: For OpenFlamingo, we expect the image to be a torch tensor of shape \n",
    " batch_size x num_media x num_frames x channels x height x width. \n",
    " In this case batch_size = 1, num_media = 3, num_frames = 1,\n",
    " channels = 3, height = 224, width = 224.\n",
    "\"\"\"\n",
    "vision_x = [image_processor(demo_image_one).unsqueeze(0), image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]\n",
    "vision_x = torch.cat(vision_x, dim=0)\n",
    "vision_x = vision_x.unsqueeze(1).unsqueeze(0)\n",
    "\n",
    "\"\"\"\n",
    "Step 3: Preprocessing text\n",
    "Details: In the text we expect an <image> special token to indicate where an image is.\n",
    " We also expect an <|endofchunk|> special token to indicate the end of the text \n",
    " portion associated with an image.\n",
    "\"\"\"\n",
    "tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "lang_x = tokenizer(\n",
    "    [\"<image>An image of two cats.<|endofchunk|><image>An image of a bathroom sink.<|endofchunk|><image>An image of\"],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 4: Generate text\n",
    "\"\"\"\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x,\n",
    "    lang_x=lang_x[\"input_ids\"],\n",
    "    attention_mask=lang_x[\"attention_mask\"],\n",
    "    max_new_tokens=20,\n",
    "    num_beams=3,\n",
    ")\n",
    "\n",
    "print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "2024-02-02 16:37:28.432095: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/user2/.cache/huggingface/modules/transformers_modules/anas-awadalla/mpt-1b-redpajama-200b/50d6bc94e17812873f39c36c5f815263fa71fb80/attention.py:289: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Flamingo.__init__() got an unexpected keyword argument 'cache_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopen_flamingo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_model_and_transforms\n\u001b[0;32m----> 3\u001b[0m model, image_processor, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model_and_transforms\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_vision_encoder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mViT-L-14\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_vision_encoder_pretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlang_encoder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manas-awadalla/mpt-1b-redpajama-200b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manas-awadalla/mpt-1b-redpajama-200b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_every_n_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPATH/TO/CACHE/DIR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Defaults to ~/.cache\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/KIJI/lib/python3.11/site-packages/open_flamingo/src/factory.py:83\u001b[0m, in \u001b[0;36mcreate_model_and_transforms\u001b[0;34m(clip_vision_encoder_path, clip_vision_encoder_pretrained, lang_encoder_path, tokenizer_path, cross_attn_every_n_layers, use_local_files, decoder_layers_attr_name, freeze_lm_embeddings, **flamingo_kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m lang_encoder\u001b[39m.\u001b[39mset_decoder_layers_attr_name(decoder_layers_attr_name)\n\u001b[1;32m     81\u001b[0m lang_encoder\u001b[39m.\u001b[39mresize_token_embeddings(\u001b[39mlen\u001b[39m(text_tokenizer))\n\u001b[0;32m---> 83\u001b[0m model \u001b[39m=\u001b[39m Flamingo(\n\u001b[1;32m     84\u001b[0m     vision_encoder,\n\u001b[1;32m     85\u001b[0m     lang_encoder,\n\u001b[1;32m     86\u001b[0m     text_tokenizer\u001b[39m.\u001b[39;49mencode(\u001b[39m\"\u001b[39;49m\u001b[39m<|endofchunk|>\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m],\n\u001b[1;32m     87\u001b[0m     text_tokenizer\u001b[39m.\u001b[39;49mencode(\u001b[39m\"\u001b[39;49m\u001b[39m<image>\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m],\n\u001b[1;32m     88\u001b[0m     vis_dim\u001b[39m=\u001b[39;49mopen_clip\u001b[39m.\u001b[39;49mget_model_config(clip_vision_encoder_path)[\u001b[39m\"\u001b[39;49m\u001b[39mvision_cfg\u001b[39;49m\u001b[39m\"\u001b[39;49m][\n\u001b[1;32m     89\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mwidth\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     90\u001b[0m     ],\n\u001b[1;32m     91\u001b[0m     cross_attn_every_n_layers\u001b[39m=\u001b[39;49mcross_attn_every_n_layers,\n\u001b[1;32m     92\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mflamingo_kwargs,\n\u001b[1;32m     93\u001b[0m )\n\u001b[1;32m     95\u001b[0m \u001b[39m# Freeze all parameters\u001b[39;00m\n\u001b[1;32m     96\u001b[0m model\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Flamingo.__init__() got an unexpected keyword argument 'cache_dir'"
     ]
    }
   ],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "\n",
    "model, image_processor, tokenizer = create_model_and_transforms(\n",
    "    clip_vision_encoder_path=\"ViT-L-14\",\n",
    "    clip_vision_encoder_pretrained=\"openai\",\n",
    "    lang_encoder_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "    tokenizer_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "    cross_attn_every_n_layers=1,\n",
    "    cache_dir=\"PATH/TO/CACHE/DIR\"  # Defaults to ~/.cache\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  \n",
      "     Think step by step to answer the question.\n",
      "     <image>\n",
      "     Question: Is the cat above a mat?\n",
      "    Program:\n",
      "    BOX0=LOC(image=IMAGE,object='cat')\n",
      "    IMAGE0=CROP_BELOW(image=IMAGE,box=BOX0)\n",
      "    BOX1=LOC(image=IMAGE0,object='mat')\n",
      "    ANSWER0=COUNT(box=BOX1)\n",
      "    ANSWER1=EVAL(expr=\"'yes' if {ANSWER0} > 0 and else 'no'\")\n",
      "    FINAL_RESULT=RESULT(var=ANSWER1)\n",
      "    <|endofchunk|>\n",
      "        <image>Question: Are there trains or fences in this scene?\n",
      "    Program:\n",
      "    BOX0=LOC(image=IMAGE,object='train')\n",
      "    BOX1=LOC(image=IMAGE,object='fence')\n",
      "    ANSWER0=COUNT(box=BOX0)\n",
      "    ANSWER1=COUNT(box=BOX1)\n",
      "    ANSWER2=EVAL(expr=\"'yes' if {ANSWER0} + {ANSWER1} > 0 else 'no'\")\n",
      "    FINAL_RESULT=RESULT(var=ANSWER)\n",
      "    <image>Question: Which place is it?\n",
      "    Program:\n",
      "    BOX0=LOC(image=IMAGE,object='place')\n",
      "    BOX1=LOC(image=IMAGE,object='house')\n",
      "    ANSWER0=COUNT(box=BOX0)\n",
      "    ANSWER1=COUNT(box=BOX1)\n",
      "    ANSWER2=EVAL(expr=\"'yes' if {ANSWER0} + {ANSWER1} > 0 else 'no'\")\n",
      "    FIN\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Step 1: Load images\n",
    "\"\"\"\n",
    "demo_image_one = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "demo_image_two = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028137.jpg\",\n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "query_image = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028352.jpg\", \n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 2: Preprocessing images\n",
    "Details: For OpenFlamingo, we expect the image to be a torch tensor of shape \n",
    " batch_size x num_media x num_frames x channels x height x width. \n",
    " In this case batch_size = 1, num_media = 3, num_frames = 1,\n",
    " channels = 3, height = 224, width = 224.\n",
    "\"\"\"\n",
    "vision_x = [image_processor(demo_image_one).unsqueeze(0), image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]\n",
    "vision_x = torch.cat(vision_x, dim=0)\n",
    "vision_x = vision_x.unsqueeze(1).unsqueeze(0)\n",
    "\n",
    "\"\"\"\n",
    "Step 3: Preprocessing text\n",
    "Details: In the text we expect an <image> special token to indicate where an image is.\n",
    " We also expect an <|endofchunk|> special token to indicate the end of the text \n",
    " portion associated with an image.\n",
    "\"\"\"\n",
    "tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "lang_x = tokenizer(\n",
    "    [\"\"\"\n",
    "     Think step by step to answer the question.\n",
    "     <image>\n",
    "     Question: Is the cat above a mat?\n",
    "    Program:\n",
    "    BOX0=LOC(image=IMAGE,object='cat')\n",
    "    IMAGE0=CROP_BELOW(image=IMAGE,box=BOX0)\n",
    "    BOX1=LOC(image=IMAGE0,object='mat')\n",
    "    ANSWER0=COUNT(box=BOX1)\n",
    "    ANSWER1=EVAL(expr=\"'yes' if {ANSWER0} > 0 and else 'no'\")\n",
    "    FINAL_RESULT=RESULT(var=ANSWER1)\n",
    "    <|endofchunk|>\n",
    "        <image>Question: Are there trains or fences in this scene?\n",
    "    Program:\n",
    "    BOX0=LOC(image=IMAGE,object='train')\n",
    "    BOX1=LOC(image=IMAGE,object='fence')\n",
    "    ANSWER0=COUNT(box=BOX0)\n",
    "    ANSWER1=COUNT(box=BOX1)\n",
    "    ANSWER2=EVAL(expr=\"'yes' if {ANSWER0} + {ANSWER1} > 0 else 'no'\")\n",
    "    FINAL_RESULT=RESULT(var=ANSWER)\n",
    "    <image>Question: Which place is it?\"\"\"],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 4: Generate text\n",
    "\"\"\"\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x,\n",
    "    lang_x=lang_x[\"input_ids\"],\n",
    "    attention_mask=lang_x[\"attention_mask\"],\n",
    "    max_new_tokens=100,\n",
    "    num_beams=3,\n",
    ")\n",
    "\n",
    "print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  \n",
      "     Think step by step to answer the question.\n",
      "     <image>\n",
      "     Question: Is the cat above a mat?\n",
      "    Program:\n",
      "    BOX0=LOC(image=IMAGE,object='cat')\n",
      "    IMAGE0=CROP_BELOW(image=IMAGE,box=BOX0)\n",
      "    BOX1=LOC(image=IMAGE0,object='mat')\n",
      "    ANSWER0=COUNT(box=BOX1)\n",
      "    ANSWER1=EVAL(expr=\"'yes' if {ANSWER0} > 0 and else 'no'\")\n",
      "    FINAL_RESULT=RESULT(var=ANSWER1)\n",
      "    <|endofchunk|>\n",
      "        <image>Question: Where is the towel?\n",
      "    Program:\n",
      "    BOX0=LOC(image=IMAGE,object='towel')\n",
      "    ANSWER0=VQA(image=IMAGE,question='What is near the towel?')\n",
      "    BOX1=LOC(image=IMAGE,object='{ANSWER0}')\n",
      "    ANSWER2=VQA(image={BOX1},question='What is this?')\n",
      "    FINAL_RESULT=RESULT(var=ANSWER2)\n",
      "    <image>Question: What is this?\n",
      "    Program:\n",
      "    BOX0=LOC(image=IMAGE,object='{ANSWER0}')\n",
      "    ANSWER0=VQA(image=IMAGE,question='What is this?')\n",
      "    BOX1=LOC(image=IMAGE,object='{ANSWER1}')\n",
      "    ANSWER2=VQA(image=IMAGE,question='What is this?')\n",
      "    FINAL_RESULT=RESULT(var=ANSWER\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Step 1: Load images\n",
    "\"\"\"\n",
    "demo_image_one = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "demo_image_two = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028137.jpg\",\n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "query_image = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028352.jpg\", \n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 2: Preprocessing images\n",
    "Details: For OpenFlamingo, we expect the image to be a torch tensor of shape \n",
    " batch_size x num_media x num_frames x channels x height x width. \n",
    " In this case batch_size = 1, num_media = 3, num_frames = 1,\n",
    " channels = 3, height = 224, width = 224.\n",
    "\"\"\"\n",
    "vision_x = [image_processor(demo_image_one).unsqueeze(0), image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]\n",
    "vision_x = torch.cat(vision_x, dim=0)\n",
    "vision_x = vision_x.unsqueeze(1).unsqueeze(0)\n",
    "\n",
    "\"\"\"\n",
    "Step 3: Preprocessing text\n",
    "Details: In the text we expect an <image> special token to indicate where an image is.\n",
    " We also expect an <|endofchunk|> special token to indicate the end of the text \n",
    " portion associated with an image.\n",
    "\"\"\"\n",
    "tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "lang_x = tokenizer(\n",
    "    [\"\"\"\n",
    "     Think step by step to answer the question.\n",
    "     <image>\n",
    "     Question: Is the cat above a mat?\n",
    "    Program:\n",
    "    BOX0=LOC(image=IMAGE,object='cat')\n",
    "    IMAGE0=CROP_BELOW(image=IMAGE,box=BOX0)\n",
    "    BOX1=LOC(image=IMAGE0,object='mat')\n",
    "    ANSWER0=COUNT(box=BOX1)\n",
    "    ANSWER1=EVAL(expr=\"'yes' if {ANSWER0} > 0 and else 'no'\")\n",
    "    FINAL_RESULT=RESULT(var=ANSWER1)\n",
    "    <|endofchunk|>\n",
    "        <image>Question: Where is the towel?\n",
    "    Program:\n",
    "    BOX0=LOC(image=IMAGE,object='towel')\n",
    "    ANSWER0=VQA(image=IMAGE,question='What is near the towel?')\n",
    "    BOX1=LOC(image=IMAGE,object='{ANSWER0}')\n",
    "    ANSWER2=VQA(image={BOX1},question='What is this?')\n",
    "    FINAL_RESULT=RESULT(var=ANSWER2)\n",
    "    <image>Question: What is this?\"\"\"],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 4: Generate text\n",
    "\"\"\"\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x,\n",
    "    lang_x=lang_x[\"input_ids\"],\n",
    "    attention_mask=lang_x[\"attention_mask\"],\n",
    "    max_new_tokens=100,\n",
    "    num_beams=3,\n",
    ")\n",
    "\n",
    "print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "GQA_CURATED_EXAMPLES=[\n",
    "\"\"\"\n",
    "Think step by step to answer the question.\n",
    "<image>\n",
    "Question: Is the vehicle in the top of the image?\n",
    "Program:\n",
    "BOX0=LOC(image=IMAGE,object='TOP')\n",
    "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
    "BOX1=LOC(image=IMAGE0,object='vehicle')\n",
    "ANSWER0=COUNT(box=BOX1)\n",
    "ANSWER1=EVAL(expr=\"'yes' if {ANSWER0} > 0 else 'no'\")\n",
    "FINAL_RESULT=RESULT(var=ANSWER1)\n",
    "<|endofchunk|>\n",
    "<image>\n",
    "Question: Who is carrying the umbrella?\n",
    "Program:\n",
    "BOX0=LOC(image=IMAGE,object='umbrella')\n",
    "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
    "ANSWER0=VQA(image=IMAGE0,question='Who is carrying the umbrella?')\n",
    "FINAL_RESULT=RESULT(var=ANSWER0)\n",
    "<|endofchunk|>\n",
    "<image>\n",
    "Question: Which place is it?\n",
    "Program:\n",
    "ANSWER0=VQA(image=IMAGE,question='Which place is it?')\n",
    "FINAL_RESULT=RESULT(var=ANSWER0)\n",
    "<|endofchunk|>\n",
    "<image>\n",
    "Question: Is the pillow in the top part or in the bottom of the picture?\n",
    "Program:\n",
    "BOX0=LOC(image=IMAGE,object='TOP')\n",
    "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
    "BOX1=LOC(image=IMAGE0,object='pillow')\n",
    "ANSWER0=COUNT(box=BOX1)\n",
    "ANSWER1=EVAL(expr=\"'top' if {ANSWER0} > 0 else 'bottom'\")\n",
    "FINAL_RESULT=RESULT(var=ANSWER1)\n",
    "<|endofchunk|>\n",
    "<image>\n",
    "Question: Which side is the food on?\n",
    "Program:\n",
    "BOX0=LOC(image=IMAGE,object='RIGHT')\n",
    "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
    "BOX1=LOC(image=IMAGE0,object='food')\n",
    "ANSWER0=COUNT(box=BOX1)\n",
    "ANSWER1=EVAL(expr=\"'right' if {ANSWER0} > 0 else 'left'\")\n",
    "FINAL_RESULT=RESULT(var=ANSWER1)\n",
    "<|endofchunk|>\n",
    "<image>\n",
    "Question: Do the post and the sign have a different colors?\n",
    "Program:\n",
    "BOX0=LOC(image=IMAGE,object='post')\n",
    "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
    "BOX1=LOC(image=IMAGE,object='sign')\n",
    "IMAGE1=CROP(image=IMAGE,box=BOX1)\n",
    "ANSWER0=VQA(image=IMAGE0,question='What color is the post?')\n",
    "ANSWER1=VQA(image=IMAGE1,question='What color is the sign?')\n",
    "ANSWER2=EVAL(expr=\"'yes' if {ANSWER0} != {ANSWER1} else 'no'\")\n",
    "FINAL_RESULT=RESULT(var=ANSWER2)\n",
    "<|endofchunk|>\n",
    "<image>\n",
    "Question: Does the traffic cone have white color?\n",
    "Program:\n",
    "BOX0=LOC(image=IMAGE,object='traffic cone')\n",
    "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
    "ANSWER0=VQA(image=IMAGE0,question='What color is the traffic cone?')\n",
    "ANSWER1=EVAL(expr=\"'yes' if {ANSWER0} == 'white' else 'no'\")\n",
    "FINAL_RESULT=RESULT(var=ANSWER1)\n",
    "<|endofchunk|>\n",
    "<image>\n",
    "Question: Are these animals of different species?\n",
    "Program:\n",
    "ANSWER0=VQA(image=IMAGE,question='Are these animals of different species?')\n",
    "FINAL_RESULT=RESULT(var=ANSWER0)\n",
    "<|endofchunk|>\n",
    "<image>\n",
    "Question: Which side of the image is the chair on?\n",
    "Program:\n",
    "BOX0=LOC(image=IMAGE,object='RIGHT')\n",
    "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
    "BOX1=LOC(image=IMAGE0,object='chair')\n",
    "ANSWER0=COUNT(box=BOX1)\n",
    "ANSWER1=EVAL(expr=\"'right' if {ANSWER0} > 0 else 'left'\")\n",
    "FINAL_RESULT=RESULT(var=ANSWER1)\n",
    "<|endofchunk|>\n",
    " <image>Question: What is this?\n",
    " Program:\n",
    "\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  \n",
      "Think step by step to answer the question.\n",
      "<image>\n",
      "Question: Is the vehicle in the top of the image?\n",
      "Program:\n",
      "BOX0=LOC(image=IMAGE,object='TOP')\n",
      "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
      "BOX1=LOC(image=IMAGE0,object='vehicle')\n",
      "ANSWER0=COUNT(box=BOX1)\n",
      "ANSWER1=EVAL(expr=\"'yes' if {ANSWER0} > 0 else 'no'\")\n",
      "FINAL_RESULT=RESULT(var=ANSWER1)\n",
      "<|endofchunk|>\n",
      "<image>\n",
      "Question: Who is carrying the umbrella?\n",
      "Program:\n",
      "BOX0=LOC(image=IMAGE,object='umbrella')\n",
      "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
      "ANSWER0=VQA(image=IMAGE0,question='Who is carrying the umbrella?')\n",
      "FINAL_RESULT=RESULT(var=ANSWER0)\n",
      "<|endofchunk|>\n",
      "<image>\n",
      "Question: Which place is it?\n",
      "Program:\n",
      "ANSWER0=VQA(image=IMAGE,question='Which place is it?')\n",
      "FINAL_RESULT=RESULT(var=ANSWER0)\n",
      "<|endofchunk|>\n",
      "<image>\n",
      "Question: Is the pillow in the top part or in the bottom of the picture?\n",
      "Program:\n",
      "BOX0=LOC(image=IMAGE,object='TOP')\n",
      "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
      "BOX1=LOC(image=IMAGE0,object='pillow')\n",
      "ANSWER0=COUNT(box=BOX1)\n",
      "ANSWER1=EVAL(expr=\"'top' if {ANSWER0} > 0 else 'bottom'\")\n",
      "FINAL_RESULT=RESULT(var=ANSWER1)\n",
      "<|endofchunk|>\n",
      "<image>\n",
      "Question: Which side is the food on?\n",
      "Program:\n",
      "BOX0=LOC(image=IMAGE,object='RIGHT')\n",
      "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
      "BOX1=LOC(image=IMAGE0,object='food')\n",
      "ANSWER0=COUNT(box=BOX1)\n",
      "ANSWER1=EVAL(expr=\"'right' if {ANSWER0} > 0 else 'left'\")\n",
      "FINAL_RESULT=RESULT(var=ANSWER1)\n",
      "<|endofchunk|>\n",
      "<image>\n",
      "Question: Do the post and the sign have a different colors?\n",
      "Program:\n",
      "BOX0=LOC(image=IMAGE,object='post')\n",
      "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
      "BOX1=LOC(image=IMAGE,object='sign')\n",
      "IMAGE1=CROP(image=IMAGE,box=BOX1)\n",
      "ANSWER0=VQA(image=IMAGE0,question='What color is the post?')\n",
      "ANSWER1=VQA(image=IMAGE1,question='What color is the sign?')\n",
      "ANSWER2=EVAL(expr=\"'yes' if {ANSWER0}!= {ANSWER1} else 'no'\")\n",
      "FINAL_RESULT=RESULT(var=ANSWER2)\n",
      "<|endofchunk|>\n",
      "<image>\n",
      "Question: Does the traffic cone have white color?\n",
      "Program:\n",
      "BOX0=LOC(image=IMAGE,object='traffic cone')\n",
      "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
      "ANSWER0=VQA(image=IMAGE0,question='What color is the traffic cone?')\n",
      "ANSWER1=EVAL(expr=\"'yes' if {ANSWER0} == 'white' else 'no'\")\n",
      "FINAL_RESULT=RESULT(var=ANSWER1)\n",
      "<|endofchunk|>\n",
      "<image>\n",
      "Question: Are these animals of different species?\n",
      "Program:\n",
      "ANSWER0=VQA(image=IMAGE,question='Are these animals of different species?')\n",
      "FINAL_RESULT=RESULT(var=ANSWER0)\n",
      "<|endofchunk|>\n",
      "<image>\n",
      "Question: Which side of the image is the chair on?\n",
      "Program:\n",
      "BOX0=LOC(image=IMAGE,object='RIGHT')\n",
      "IMAGE0=CROP(image=IMAGE,box=BOX0)\n",
      "BOX1=LOC(image=IMAGE0,object='chair')\n",
      "ANSWER0=COUNT(box=BOX1)\n",
      "ANSWER1=EVAL(expr=\"'right' if {ANSWER0} > 0 else 'left'\")\n",
      "FINAL_RESULT=RESULT(var=ANSWER1)\n",
      "<|endofchunk|>\n",
      " <image>Question: What is this?\n",
      " Program:\n",
      " ANSWER0=VQA(image=IMAGE,question='What is this?')\n",
      " FINAL_RESULT=RESULT(var=ANSWER0)\n",
      "<|endofchunk|>\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "from functools import partial\n",
    "\n",
    "# sys.path.append('/home/user2/code/WIL_DeepLearningProject_2/VisualProgramming')\n",
    "\n",
    "# from prompt.gqa import create_prompt\n",
    "\n",
    "image_id = ['n437038', 'n153818', 'n542565', 'n428090', 'n435687', 'n167620', 'n70342', 'n418470', 'n77875']\n",
    "\n",
    "vision_x = []\n",
    "\n",
    "image_path = '/data2/NS/GQA/images/images/'\n",
    "for i in image_id:\n",
    "    demo_image = Image.open(os.path.join(image_path,i+\".jpg\")) \n",
    "    vision_x.append(image_processor(demo_image).unsqueeze(0))\n",
    "    \n",
    "query_image = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028352.jpg\", \n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "vision_x.append(image_processor(query_image).unsqueeze(0))\n",
    "\n",
    "vision_x = torch.cat(vision_x, dim=0)\n",
    "vision_x = vision_x.unsqueeze(1).unsqueeze(0)\n",
    "question = \"What is this?\"\n",
    "\n",
    "tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "lang_x = tokenizer(\n",
    "   GQA_CURATED_EXAMPLES,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x,\n",
    "    lang_x=lang_x[\"input_ids\"],\n",
    "    attention_mask=lang_x[\"attention_mask\"],\n",
    "    max_new_tokens=100,\n",
    "    num_beams=3,\n",
    ")\n",
    "\n",
    "print(\"Generated text: \", tokenizer.decode(generated_text[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('KIJI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d4ce9b068c66b067ddc9a0e35241e261eb403701c001fd5e69fb192b3569b41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
