{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86cee2e0-9079-4c23-8181-873ef564978d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 13:37:48.063317: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-13 13:37:48.204370: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-13 13:37:48.220944: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-13 13:37:48.759063: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.7/lib64:/usr/local/cuda-11.2/lib64::/usr/local/cuda-11.2/lib64\n",
      "2023-12-13 13:37:48.792772: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.7/lib64:/usr/local/cuda-11.2/lib64::/usr/local/cuda-11.2/lib64\n",
      "2023-12-13 13:37:48.792798: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (LlamaForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88b220a1-f250-4b42-ac1d-25ca358f49bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model from HuggingFace hub\n",
    "base_model = \"kfkas/Llama-2-ko-7b-Chat\"\n",
    "\n",
    "# New instruction dataset\n",
    "dataset = load_dataset(\"squad_kor_v2\")\n",
    "\n",
    "# Fine-tuned model\n",
    "new_model = \"llama-2-7b-ko-wikidata-QA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145db52c-f309-408b-9202-25ad1d56dc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answer', 'url', 'raw_html'],\n",
       "        num_rows: 83486\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answer', 'url', 'raw_html'],\n",
       "        num_rows: 10165\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a155847-4f40-49aa-8d6b-198a2d056ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3929/1955066438.py:7: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  trainset['text'] = \"<s>[INST] \" + trainset['question'].astype(str) + \"[/INST]\" + trainset['answer'].apply(lambda x: x['text']).apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
      "/tmp/ipykernel_3929/1955066438.py:7: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  trainset['text'] = \"<s>[INST] \" + trainset['question'].astype(str) + \"[/INST]\" + trainset['answer'].apply(lambda x: x['text']).apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
      "/tmp/ipykernel_3929/1955066438.py:10: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  testset['text'] = \"<s>[INST] \" + testset['question'].astype(str) + \"[/INST]\" + testset['answer'].apply(lambda x: x['text']).apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 83486\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Set Train/Test Sets\n",
    "trainset = dataset['train'].to_pandas()\n",
    "testset = dataset['validation'].to_pandas()\n",
    "\n",
    "# Preprocessing\n",
    "# Trainset\n",
    "trainset['text'] = \"<s>[INST] \" + trainset['question'].astype(str) + \"[/INST]\" + trainset['answer'].apply(lambda x: x['text']).apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "\n",
    "# Testset\n",
    "testset['text'] = \"<s>[INST] \" + testset['question'].astype(str) + \"[/INST]\" + testset['answer'].apply(lambda x: x['text']).apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "\n",
    "temp_train = trainset['text'].reset_index()\n",
    "temp_test = testset['text'].reset_index()\n",
    "\n",
    "del temp_train['index']\n",
    "del temp_test['index']\n",
    "\n",
    "tds = Dataset.from_pandas(temp_train)\n",
    "vds = Dataset.from_pandas(temp_test)\n",
    "\n",
    "dataset = DatasetDict()\n",
    "\n",
    "dataset['train'] = tds\n",
    "dataset['validation'] = vds\n",
    "\n",
    "print(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb5431a-e8a9-493d-8d0b-69ffe23e4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization configuration\n",
    "\"\"\"\n",
    "4-bit quantization via QLoRA allows efficient finetuning of huge LLM models on consumer hardware while retaining high performance. This dramatically improves accessibility and usability for \n",
    "real-world applications.\n",
    "QLoRA quantizes a pre-trained language model to 4 bits and freezes the parameters. A small number of trainable low-rank adapter layers are then added to the model.\n",
    "\n",
    "During fine-tuning, gradients are backpropagated through the frozen 4-bit quanized model into only the Low-Rank Adapter layers. So, the entire pretrained model remains fixed at\n",
    "4-bits while only the adapters are updated. Also, the 4-bit quantization does not hurt model performance.\n",
    "\"\"\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = compute_dtype,\n",
    "    bnb_4bit_use_double_quant = False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "648aca29-d0e7-499e-bae5-54328b854e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80998b017aa341d4af2a7a2f9f7adacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be424bc621d34dcbaa176d4a30649f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9c153911e5466aa6da6a5d87898adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78be98b5e14143e1838e700aad3d1c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093624f6d9af4c24b2c33a81ffc11c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c92ed6df6e420082be535becf9e85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64dcb3ea0d0446aaf3a9b9442128b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fa91be9e8e4625b92eabc81596688d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/5.21M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6738a90b13734bcdb1cd92c918909fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading Llama 2 model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config = quant_config,\n",
    "    device_map = {\"\" : 0}\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.padding_side = \"right\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9db1f54-c105-4d72-b3c4-01d499a63056",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_params = LoraConfig(\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.1,\n",
    "    r = 32,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Training Model config\n",
    "training_params = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    optim = \"paged_adamw_32bit\",\n",
    "    save_steps = 25,\n",
    "    logging_steps = 25,\n",
    "    learning_rate = 2e-4,\n",
    "    weight_decay = 0.001,\n",
    "    fp16 = False,\n",
    "    bf16 = False,\n",
    "    max_grad_norm = 0.3,\n",
    "    max_steps = -1,\n",
    "    warmup_ratio = 0.03,\n",
    "    group_by_length = True,\n",
    "    lr_scheduler_type = \"constant\",\n",
    "    report_to = \"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a630f02-0bd7-4fcd-b68f-e292741b4be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49566e22d73a42d596c06a924c8d2bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/83486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcab121be29a479d86708d8ed9a29e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset['train'],\n",
    "    eval_dataset = dataset['validation'],\n",
    "    peft_config = peft_params,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 512,\n",
    "    tokenizer = tokenizer,\n",
    "    args = training_params,\n",
    "    packing = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b893dfd-4001-491e-8705-34415e170ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. It is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnova18\u001b[0m (\u001b[33mandlabyonsei\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/andlab/wandb/run-20231107_192439-nxk7fftn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andlabyonsei/huggingface/runs/nxk7fftn' target=\"_blank\">apricot-mountain-111</a></strong> to <a href='https://wandb.ai/andlabyonsei/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andlabyonsei/huggingface' target=\"_blank\">https://wandb.ai/andlabyonsei/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andlabyonsei/huggingface/runs/nxk7fftn' target=\"_blank\">https://wandb.ai/andlabyonsei/huggingface/runs/nxk7fftn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c2c886d2dd4dcfad83618a280531ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2142, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.4234, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.3469, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.13, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.3052, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.252, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.2845, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.0892, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.0099, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0313, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.1509, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.053, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0869, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0305, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.211, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0023, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0869, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.117, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.2594, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9673, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.1588, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.1039, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0961, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.077, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0787, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0472, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.1853, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.029, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.1709, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0702, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.1403, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0927, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.062, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9849, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.1593, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9955, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.171, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0325, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9714, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0598, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0804, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9577, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9034, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.996, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9385, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9133, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0597, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0288, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9505, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0981, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9503, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.1394, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0839, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9678, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0362, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0514, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.2111, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9627, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9991, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.8969, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0156, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0541, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.1194, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0841, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0353, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0457, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.8266, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0348, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0651, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.049, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.1334, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0124, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.986, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9222, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9854, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9979, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0103, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.1106, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9668, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9036, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9611, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0177, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0483, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0032, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.1645, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0409, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.1125, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.886, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9912, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.1106, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0249, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9274, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9168, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0618, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0034, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.098, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0833, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9389, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9261, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0298, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0219, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0641, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0301, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.97, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0968, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9695, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9637, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9782, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9095, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9379, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.1486, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.1357, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9138, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.8589, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.905, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0352, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.036, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0153, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.073, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.97, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0679, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0141, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9666, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9192, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.143, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0685, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.1187, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9453, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.991, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9412, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.1474, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.8535, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0152, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.8946, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9423, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9836, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0121, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9816, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0535, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0464, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9933, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0111, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 2.1067, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 2.0235, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 2.0543, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9514, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8975, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 2.0235, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 2.0301, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 2.0682, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9389, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9922, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8757, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9426, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9417, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 2.0491, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9791, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 2.0038, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 2.0326, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9662, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9642, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8967, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9696, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 2.0208, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9137, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9671, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 2.1153, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 2.0094, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9148, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9951, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9755, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8907, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9334, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8331, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9174, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8803, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.967, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 2.03, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8642, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8125, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 2.0454, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9331, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9983, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9759, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 2.0557, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.905, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 2.072, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 2.0619, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 2.0602, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9237, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9144, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9096, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 2.1957, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9858, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8757, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9323, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 2.031, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9547, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9948, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9104, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9147, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 2.0007, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 2.0493, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 2.0145, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9957, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8779, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.934, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.821, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 2.0948, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 2.0214, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.9844, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.8921, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.8665, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.9, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.9087, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.9793, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.9553, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.8529, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 2.0247, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.9002, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.9167, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.8289, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 2.0747, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.8667, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.9881, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 2.0646, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 1.9976, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 2.0188, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 1.9368, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 2.0042, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 2.0618, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 1.9718, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 2.0333, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 2.0052, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 1.9884, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 1.9349, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 1.9565, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 1.8974, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 2.0515, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 1.9875, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 2.0815, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 1.8939, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 1.9809, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 2.0043, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 1.9338, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 1.9446, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 1.8952, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 1.8431, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 2.0061, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 1.8994, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 1.8417, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 1.9761, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 1.8512, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 1.911, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 2.0815, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 1.9263, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 1.8944, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 2.0023, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 1.8665, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 1.8905, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 1.9319, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 1.9173, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 2.0085, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 1.7851, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 2.2402, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 1.9214, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 1.9209, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 1.9668, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 1.9509, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 1.8635, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 2.085, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 1.899, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 1.9197, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 1.9077, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 1.9162, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 1.9242, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 2.032, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 1.9124, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 2.1056, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 2.0045, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 2.0571, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 2.0251, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 2.0733, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 1.8837, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 2.0621, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 1.8372, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 1.948, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 1.7961, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 1.8174, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 1.8894, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 2.0978, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 2.0093, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 1.9664, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 1.7966, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 1.895, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 1.884, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 2.0504, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 2.1281, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 2.0098, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 2.0268, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 2.0744, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 1.9894, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 2.0094, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 1.9341, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 2.0821, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 1.9699, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 2.0278, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 1.9257, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 1.9863, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 2.0342, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 1.8885, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 1.8752, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 1.9312, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 1.9604, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 1.9597, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 1.8824, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 1.971, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 2.007, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 1.7927, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 1.935, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 1.9954, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 1.9184, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 2.0528, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 1.9144, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 1.8481, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 1.8208, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 1.9472, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 1.8404, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 1.908, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 1.9113, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 2.0622, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 1.95, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 1.835, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 2.0243, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 1.9849, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 1.9261, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 1.8956, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 1.9379, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 2.0144, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 1.9065, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 2.0026, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 1.9134, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 1.9604, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 2.0378, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 2.1058, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 1.8681, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 1.9953, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 1.8472, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 2.0135, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 2.0499, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 1.9237, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 2.0336, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 2.0828, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 1.9277, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 2.0155, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 1.863, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 2.078, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 1.9638, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 1.9586, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 1.9427, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 2.0142, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 1.9413, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 2.1081, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 1.9406, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 1.8956, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 1.8992, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 1.9517, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 1.9459, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 2.1051, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 1.9549, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 1.9086, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 1.954, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 2.0404, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 1.7776, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 1.9407, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 1.8516, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 2.1108, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 1.9925, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 1.9594, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 1.8537, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 1.847, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 1.8672, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 1.8872, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 1.91, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 1.9981, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 1.814, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 1.9498, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 1.9814, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 1.8728, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 1.8295, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 2.0288, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 1.9442, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 2.061, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 1.8524, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 2.0033, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 1.8975, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 1.9889, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 1.8373, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 2.0205, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 1.888, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 1.975, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 1.9577, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 2.0265, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 1.8941, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 2.0282, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 1.9005, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 2.0728, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 1.8896, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 2.0457, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 1.8603, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 1.9051, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 1.8654, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 1.929, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 1.945, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 2.0778, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 1.9055, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 1.9808, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 1.8228, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 1.9851, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 1.9789, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 1.9186, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 1.9163, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 1.9738, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 1.9746, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 1.9643, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 1.9287, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 1.8826, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 1.9163, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 1.9294, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 1.8169, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 1.88, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 1.9306, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 2.0054, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 1.817, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 1.9316, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 1.9509, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 2.0628, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 1.8656, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 1.8852, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 1.9242, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 1.8604, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 2.0049, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 1.9019, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 1.8952, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 1.8154, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 2.0544, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 1.8654, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 1.9932, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 1.9761, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 1.8715, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 1.9871, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 1.9085, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 1.9943, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 1.9374, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 1.9407, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 1.8064, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 2.0153, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 1.8829, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 1.9212, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 1.8923, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 1.9697, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 1.9899, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 2.127, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 1.9308, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 1.7515, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 1.8913, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 2.0981, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 1.8685, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 2.0138, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 1.9161, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 1.9948, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 1.9153, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 2.1877, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 1.9387, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 1.8938, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 1.9178, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 1.9322, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 1.9031, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 2.0232, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 1.8661, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 1.9726, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 1.8938, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 1.9045, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 1.9682, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 1.9855, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 1.8602, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 1.9433, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 1.8048, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 1.7904, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 1.9182, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 1.8904, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 1.8506, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 1.9539, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 1.8575, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 2.0674, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 1.7911, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 2.0062, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 1.9572, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 2.0095, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 1.9134, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 1.9969, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 1.8698, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 1.8757, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 1.9587, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 1.7978, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 1.9281, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 1.8686, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 1.7918, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 1.871, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 1.8953, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 1.9441, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 1.8769, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 1.9216, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 1.9467, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 2.0748, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 1.9361, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 1.8678, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 1.9246, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 2.0544, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 1.9573, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 2.1086, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 1.9547, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 1.8058, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 1.8864, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 1.7788, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 1.8909, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 1.9479, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 1.8539, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 1.9416, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 1.8369, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 1.9008, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 1.9499, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 1.9066, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 1.8954, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 2.0022, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 2.051, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 1.9097, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 1.8763, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 1.8677, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 1.884, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 1.8961, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 2.0101, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 1.8923, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 1.9026, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 1.9999, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 1.8575, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 1.916, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 1.8414, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 1.8756, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 1.7593, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 1.8587, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 2.0148, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 1.9191, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 1.8276, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 2.0129, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 1.8688, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 1.8621, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 1.8904, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 1.8806, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 1.8543, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 1.8437, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 1.8291, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 2.0668, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 1.8814, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 1.896, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 2.0158, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 2.0285, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 1.8551, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 1.992, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 1.8983, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 1.84, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 1.8666, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 2.0706, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 1.8496, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 1.9242, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 1.857, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 1.895, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 1.9538, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 1.9607, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 1.991, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 1.8954, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 1.9824, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 2.0258, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 1.872, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 2.1144, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 1.8027, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 2.0017, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 1.9131, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 2.0551, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 1.837, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 1.8005, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 1.8885, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 2.0204, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 1.824, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 1.7435, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 1.9982, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 2.013, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 1.9204, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 2.0229, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 2.0304, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 1.9026, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 1.995, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 2.0337, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 1.8218, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 2.0439, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 1.8678, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 2.0536, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 1.9965, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 1.9626, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 1.9338, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 2.0588, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 1.9099, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 1.9351, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 1.9791, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 1.8751, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 1.9926, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 1.8502, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 1.968, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 1.9129, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 1.868, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 1.9685, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 1.8746, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 1.9741, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 1.9244, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 1.9435, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 1.9216, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 2.0137, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 1.8063, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 1.8603, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 1.9082, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 1.9527, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 2.0184, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 2.0871, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 1.8247, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 1.9726, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 1.9489, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 1.9658, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 1.8673, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 1.8192, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 1.8462, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 1.8716, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 1.87, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 2.049, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 1.8521, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 2.0654, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 1.9287, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 1.8392, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 1.8196, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 1.8796, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 1.9963, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 1.949, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 1.8828, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 2.0847, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 1.7667, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 1.9759, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 1.7705, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 2.0087, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 1.8229, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 1.9277, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 1.7451, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 1.9773, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 1.8137, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 1.9267, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 2.0008, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 1.8974, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 1.8348, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 2.0158, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 1.8434, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 1.9932, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 1.8738, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 1.7519, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 1.9083, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 1.9581, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 2.0022, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 1.9121, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 1.8297, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 1.9894, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 1.812, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 2.0049, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 1.9513, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 1.9051, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 1.9412, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 1.8893, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 1.75, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 1.8844, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 1.8625, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 1.9683, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 1.9554, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 1.9448, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 1.9619, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 1.8522, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 1.8274, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 1.9589, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 1.9143, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 1.8468, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 1.8536, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 2.0093, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 1.829, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 1.8281, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 1.7568, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 1.8588, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 1.9113, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 2.0455, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 1.8431, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 1.8741, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 1.8486, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 2.0295, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 1.9012, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 1.8593, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 1.8126, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 2.2169, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 1.9543, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 1.8705, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 1.8928, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 1.8349, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 1.9072, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 1.8938, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 1.7721, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 1.9409, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 2.0084, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 1.9511, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 1.8517, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 2.0872, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 1.9005, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 1.8338, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 2.0395, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 2.0117, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 1.8563, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 1.91, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 1.9161, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 1.9846, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 1.9025, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 1.8534, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 1.9133, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 1.8896, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 1.882, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 1.9608, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 1.9186, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 1.9847, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 1.8207, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 1.8676, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 1.8377, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 1.9311, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 1.9292, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 2.0619, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 1.9794, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 1.9654, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 1.8553, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 1.8613, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 2.0172, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 2.0678, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 1.9108, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 1.8661, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 1.9068, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 1.846, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 2.1264, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 1.8643, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 1.9374, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 1.8544, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 1.8287, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 1.9511, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 2.0372, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 1.8883, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 1.8314, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 1.9557, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 1.9406, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 1.9305, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 2.013, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 1.9554, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 1.9121, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 1.8917, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 1.932, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 1.8781, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 2.1266, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 2.0649, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 1.9075, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 1.8929, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 1.8889, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 2.0443, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 1.9866, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 1.8745, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 1.8047, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 1.9285, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 1.8668, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 1.7988, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 1.8222, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 1.9147, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 1.8156, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 1.9004, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 1.8816, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 1.9259, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 1.7992, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 1.9434, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 1.9551, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 1.9449, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 1.8992, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 1.8988, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 1.9358, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 2.023, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 1.8507, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 1.7753, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 1.875, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 2.0763, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 1.8613, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 1.8061, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 1.9599, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 2.1249, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 1.8984, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 1.9669, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 1.9663, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 1.7821, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 1.912, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 1.8875, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 1.8907, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 1.876, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 1.9777, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 2.057, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 1.7998, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 2.0475, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 1.9992, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 2.1604, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 1.8601, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 1.8809, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 1.814, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 1.9139, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 1.7438, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 2.1008, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 1.9391, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 1.8064, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 1.8505, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 1.9833, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 1.7915, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 1.9027, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 1.7877, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 1.9274, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 1.9374, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 1.9133, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 2.0533, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 1.9376, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 1.8308, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 1.8038, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 1.8934, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 1.973, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 1.8726, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 1.8414, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 1.9907, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 2.0127, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 1.8298, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 1.9555, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 1.948, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 1.883, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 2.0004, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 1.8884, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 1.8843, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 1.8308, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 1.818, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 1.9551, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 1.8763, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 2.0844, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 1.7891, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 1.9708, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 1.8423, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 1.837, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 1.8957, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 2.058, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 1.7115, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 1.9092, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 1.7952, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 1.9583, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 1.9478, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 1.9095, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 1.9011, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 1.9305, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 1.7476, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 1.739, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 1.8941, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 2.0108, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 1.7732, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 1.996, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 1.9171, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 1.9429, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 1.8557, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 1.77, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 1.8745, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 1.9411, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 1.8801, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 1.9535, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 1.8682, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 1.9392, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 1.8452, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 1.8693, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 1.8157, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 2.0237, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 1.8754, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 1.8916, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 1.9855, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 2.0404, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 1.8287, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 1.9377, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 1.853, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 1.8963, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 1.8304, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 1.9155, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 1.7308, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 1.9437, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 1.9375, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 1.7459, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 2.0643, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 1.9946, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 1.7999, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 1.8514, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 1.7842, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 2.005, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 1.8628, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 1.7976, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 1.9291, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 1.8581, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 1.9457, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 1.9465, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 1.9308, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 1.8665, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 1.7443, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 1.9154, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 1.7406, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 1.9132, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 1.9073, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 1.8879, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 1.8805, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 1.8403, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 1.7942, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 1.9334, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 2.0459, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 1.9208, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 1.8594, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 1.8796, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 1.8906, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 1.9345, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 1.8288, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 2.0209, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 1.8592, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 2.0043, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 1.9684, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 2.0363, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 1.8124, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 1.9769, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 1.9975, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 1.8763, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 1.9207, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 2.0628, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 1.7747, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 1.8126, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 1.8612, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 1.9733, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 1.8947, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 1.8079, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 2.0166, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 1.9514, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 1.8204, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 1.9012, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 1.8699, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 2.0776, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 1.7457, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 2.0842, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 1.9466, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 1.9563, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 1.7726, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 1.958, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 1.9244, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 1.9074, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 1.8734, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 1.9397, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 1.8173, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 1.8304, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 1.8986, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 1.9091, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 1.8849, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 1.9124, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 1.9287, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 1.9568, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 1.9391, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 1.7604, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 1.9585, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 1.7705, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 1.8241, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 1.74, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 1.8232, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 1.8811, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 1.9216, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 2.0372, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 1.8666, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 1.8126, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 1.9144, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 1.854, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 1.9788, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 1.9929, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 1.7557, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 1.9209, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 1.8599, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 1.8032, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 1.7543, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 1.8938, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 1.8528, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 1.9236, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 1.8831, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 1.9893, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 1.8732, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 1.8139, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 1.8765, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 1.9666, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 1.8474, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 1.9904, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 1.8573, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 2.0, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 1.8205, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 1.8737, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 1.8622, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 2.0227, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 1.7962, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 2.0516, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 1.7802, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 2.0013, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 1.8326, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 2.0057, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 1.9548, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 1.9134, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 1.8551, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 1.8704, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 1.949, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 2.0246, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 1.7684, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 2.0704, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 1.8685, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 1.8596, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 1.8992, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 2.0781, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 1.8456, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 2.1889, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 1.8928, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 1.9283, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 1.8415, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 1.8683, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 1.8842, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 2.0223, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 1.8914, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 2.0086, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 1.7758, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 1.8856, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 1.7903, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 1.9252, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 1.9987, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 1.9887, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 1.7817, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 1.9359, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 1.9049, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 1.8104, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 1.9114, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 1.74, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 1.8641, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 2.0583, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 1.86, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 2.0585, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 1.7184, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 1.8707, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 1.8652, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 1.8968, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 1.8453, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 2.0851, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 1.961, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 2.0566, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 1.9812, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 1.8975, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 1.8691, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 1.9133, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 1.954, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 1.7913, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 1.9589, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 1.8384, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 1.7919, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 2.0136, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 1.6816, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 1.7889, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 1.718, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 1.92, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 1.9728, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 1.8274, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 1.8339, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 1.9092, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 1.8177, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 1.8717, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 1.8605, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 1.9144, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 1.8142, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 1.7055, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 1.8057, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 1.9208, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 1.8001, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 1.8911, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 1.9329, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 1.7677, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 1.8856, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 1.9346, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 1.7849, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 1.9415, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 1.9061, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 1.9814, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 1.7864, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 1.9076, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 1.7537, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 1.9998, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 1.8506, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 1.8497, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 1.9321, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 1.8938, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 1.9396, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 1.6785, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 1.8985, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 1.8324, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 1.954, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 1.8459, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 1.7792, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 1.9608, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 1.8315, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 2.0435, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 1.8932, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 1.8807, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 1.9732, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 1.9109, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 1.7514, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 1.8054, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 1.8414, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 1.9076, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 1.8884, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 1.9955, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 1.8448, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 1.865, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 1.9114, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 2.0081, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 1.8477, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 1.9599, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 1.9071, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 2.0962, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 1.8174, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 1.8638, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 1.8277, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 1.7998, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 1.8957, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 1.9245, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 1.8713, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 1.9285, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 1.936, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 1.9362, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 1.794, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 2.0222, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 1.9535, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 2.0532, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 1.7978, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 1.9375, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 1.962, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 2.0021, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 1.8855, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 1.7466, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 1.8608, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 1.8802, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 1.8499, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 1.9328, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 1.76, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 1.9643, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 1.9175, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 1.9222, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 1.8501, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 1.9509, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 1.8978, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 2.0163, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 1.9593, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 1.8855, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 1.9039, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 1.9021, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 1.7744, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 1.9814, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 1.799, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 1.7213, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 1.8864, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 1.9094, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 1.7637, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 1.9802, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 1.8499, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 2.0783, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 1.7801, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 1.924, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 1.8147, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 1.9816, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 2.0322, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 1.889, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 1.8301, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 1.9196, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 1.9781, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 1.9005, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 1.8689, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 1.9338, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 1.8501, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 1.9496, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 1.8454, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 1.903, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 1.9126, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 1.8094, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 1.7606, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 1.9499, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 1.7947, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 1.9676, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 1.7621, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 1.8408, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 1.8881, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 1.8923, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 1.8214, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 1.9047, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 1.7439, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 1.9491, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 1.9324, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 2.0029, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 1.9045, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 1.9982, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 1.9406, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 1.9274, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 1.9813, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 1.9372, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 1.9439, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 1.9219, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 1.8319, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 1.9825, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 1.7061, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 2.0096, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 1.8628, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 2.0494, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 1.9553, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 1.7534, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 1.8126, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 1.9869, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 1.8499, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 1.9723, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 1.8854, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 1.947, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 1.7446, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 1.9953, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 1.8503, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 1.9075, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 1.9876, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 1.9976, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 1.7921, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 2.1056, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 1.8711, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 1.8067, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 1.8192, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 1.9924, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 1.8339, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 1.8025, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 1.7338, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 1.8988, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 1.8353, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 1.9813, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 1.8748, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 2.0473, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 1.8216, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 1.8856, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 1.8807, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 1.7618, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 1.9516, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 1.9257, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 1.8075, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 2.0971, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 2.0011, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 2.0435, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 1.8326, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 1.7908, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 1.8962, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 1.891, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 1.8228, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 1.89, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 1.8599, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 1.8886, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 1.8489, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 1.9512, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 1.7457, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 1.92, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 1.9934, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 1.8038, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 1.7847, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 1.96, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 2.0207, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 1.8223, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 1.9118, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 1.8621, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 1.7996, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 1.8566, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 1.8886, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 1.7902, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 1.9785, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 1.9298, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 1.846, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 1.9049, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 1.8898, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 1.9202, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 1.7941, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 1.9615, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 1.843, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 1.7333, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 1.7891, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 2.1326, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 1.758, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 1.9643, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 1.9165, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 1.8192, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 2.0356, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 2.0154, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 1.7918, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 1.9029, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 1.8531, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 1.8491, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 1.935, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 1.9247, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 1.9262, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 2.0382, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 1.8244, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 1.8722, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 1.811, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 1.8899, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 1.8598, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 1.8866, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 1.8061, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 1.7582, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 1.8428, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 1.855, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 1.8138, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 1.8342, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 1.871, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 1.9142, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 1.9359, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 1.9344, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 1.8888, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 2.0509, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 1.9111, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 2.0381, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 1.8899, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 1.9255, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 1.8377, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 1.8022, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 1.8252, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 1.9036, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 1.7121, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 1.9545, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 1.8066, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 2.0042, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 1.8371, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 1.828, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 1.8119, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 2.0522, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 1.867, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 1.7666, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 2.0649, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 1.7888, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 1.8708, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 1.9533, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 1.6658, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 1.9515, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 1.7724, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 1.9018, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 1.7164, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 1.8327, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 1.7945, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 1.8132, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 1.7721, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 1.977, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 1.8858, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 1.9601, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 1.8476, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 1.806, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 2.0235, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 1.9709, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 1.681, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 1.9362, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 1.8628, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 1.8039, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 1.8882, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 2.0715, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 1.8315, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 1.7897, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 1.8577, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 1.9517, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 1.8916, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 1.7391, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 1.8921, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 1.9002, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 1.9345, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 1.8769, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 1.8344, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 1.8566, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 1.8518, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 1.9026, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 1.8667, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 1.9442, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 1.9362, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 1.8747, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 1.7219, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 1.9067, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 2.0099, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 1.9199, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 1.9635, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 1.987, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 1.8614, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 1.8543, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 1.7394, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 1.8669, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 1.9642, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 1.8716, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 1.9368, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 1.9323, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 1.8745, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 1.925, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 1.709, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 1.9533, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 1.8709, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 2.0892, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 1.832, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 1.9901, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 1.9014, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 1.7907, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 1.9489, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 1.9116, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 1.9653, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 1.8668, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 1.9852, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 1.9176, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 1.8483, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 1.961, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 1.9124, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 1.9304, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 1.881, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 1.7901, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 1.7373, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 1.9648, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 1.8692, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 1.927, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 1.8972, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 2.0077, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 1.8432, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 1.9182, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 1.831, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 1.7684, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 1.8622, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 1.987, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 1.8703, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 1.9043, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 1.9437, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 1.7551, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 1.9597, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 2.0669, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 1.9376, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 1.8422, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 1.8714, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 1.9127, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 1.8776, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 2.0478, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 1.8485, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 1.8052, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 1.7438, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 2.0047, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 1.8116, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 2.0184, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 1.8593, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 1.9403, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 1.9377, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 1.9555, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 1.8403, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 1.9851, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 1.8338, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 1.8976, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 1.8427, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 1.8466, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 1.8215, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 1.8411, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 1.8855, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 1.9833, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 1.8023, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 1.8467, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 1.7395, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 1.9608, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 1.9186, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 2.0593, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 1.8545, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 1.9704, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 1.8549, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 1.8475, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 1.9098, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 2.0473, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 1.8651, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 1.9966, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 1.9345, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 1.9487, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 1.811, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 1.9495, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 1.9008, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 1.7168, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 1.9145, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 1.9172, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 1.9691, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 1.8309, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 1.8937, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 1.9582, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 1.9019, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 1.9536, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 1.8777, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 1.9478, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 1.8014, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 1.8516, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 1.6915, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 1.8284, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 1.9569, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 1.8827, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 1.7932, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 1.8505, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 1.9152, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 1.8955, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 1.8011, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 2.0446, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 1.9211, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 1.8444, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 1.8753, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 2.0454, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 1.788, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 1.9573, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 1.877, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 1.9469, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 1.7944, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 1.9622, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 1.8877, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 1.9675, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 1.8419, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 2.0144, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 1.7956, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 1.8214, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 1.7895, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 1.8296, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 1.8251, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 2.006, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 1.8195, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 2.0334, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 2.0464, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 1.9765, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 1.8909, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 1.7906, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 1.8574, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 1.9697, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 1.8038, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 2.0048, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 1.8739, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 1.9937, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 1.7803, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 1.9102, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 1.9726, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 1.8714, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 1.9263, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 1.8756, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 1.7777, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 1.8417, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 1.8769, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 1.9778, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 1.8954, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 2.0528, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 1.8044, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 1.8869, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 1.8251, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 1.973, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 1.8827, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 2.0491, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 1.7521, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 1.8967, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 1.8457, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 1.9214, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 1.8988, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 2.0369, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 1.6944, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 1.9385, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 1.748, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 2.0324, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 1.8934, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 2.0284, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 1.7443, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 2.0076, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 1.8812, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 1.9793, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 1.78, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 1.7597, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 1.7945, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 2.0376, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 1.7885, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 1.8299, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 1.7353, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 1.9197, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 1.8715, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 1.8919, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 1.7675, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 1.9272, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 2.0215, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 1.9316, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 1.7957, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 1.7783, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 1.9267, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 1.8434, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 1.8425, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 1.8617, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 1.8251, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 1.9886, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 1.7811, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 2.0528, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 1.8145, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 1.981, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 1.9676, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 1.8404, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 1.9071, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 2.0204, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 1.7382, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 1.9181, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 1.7742, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 2.1257, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 1.8786, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 1.8331, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 1.8305, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 1.9772, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 1.6834, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 1.877, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 1.7605, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 1.9335, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 1.8447, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 1.7105, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 1.9021, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 1.8438, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 1.8698, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 1.8922, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 1.8214, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 1.9695, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 1.7886, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 1.9808, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 1.8331, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 1.6999, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 1.8163, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 1.8584, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 1.902, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 1.8813, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 1.8076, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 1.9868, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 1.7759, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 1.9398, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 1.9491, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 1.7284, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 1.7824, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 2.0181, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 1.8952, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 1.7416, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 1.8283, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 1.8496, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 1.8649, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 1.9455, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 1.8503, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      "{'loss': 1.881, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      "{'loss': 1.8166, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      "{'loss': 1.9144, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      "{'loss': 1.8928, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      "{'loss': 1.9512, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      "{'loss': 1.8955, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      "{'loss': 1.8566, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      "{'train_runtime': 26663.7045, 'train_samples_per_second': 3.131, 'train_steps_per_second': 1.566, 'train_loss': 1.9224704369305394, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=41743, training_loss=1.9224704369305394, metrics={'train_runtime': 26663.7045, 'train_samples_per_second': 3.131, 'train_steps_per_second': 1.566, 'train_loss': 1.9224704369305394, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01e6a51a-2a96-446b-aa40-364425f18301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama-2-7b-ko-wikidata-QA/tokenizer_config.json',\n",
       " 'llama-2-7b-ko-wikidata-QA/special_tokens_map.json',\n",
       " 'llama-2-7b-ko-wikidata-QA/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09912045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e624a7e7954527af0e2c5b2adb25eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "\n",
    "# 저장된 모델과 토크나이저를 불러옵니다.\n",
    "model = LlamaForCausalLM.from_pretrained('./llama-2-7b-ko-wikidata-QA')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./llama-2-7b-ko-wikidata-QA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62f9f64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] 레오나르도 다빈치는 누구야? [/INST]레오나르도다빈치(Leonardo da Vinci)는 1452년 4월 15일피렌체에서태어났다.그의아버지는피렌체의가죽공예가였다.다빈치는 1466년 14세때피렌체의안드레아델베르치(Andrea del Verrocchio)의화실에서도제수업을받았다.그의스승은피렌체의명문가문출신으로,다빈치보다 10년정도연상이었다.다빈치는스승의딸과결혼하였다.다빈치는 1472년 10세때피렌체의산로렌초성당(Basilica di San Lorenzo)에있는마르티노델리(Martino del\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = \"레오나르도 다빈치는 누구야?\"\n",
    "pipe = pipeline(task = \"text-generation\", model = model, tokenizer = tokenizer, max_length = 200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f2ae93a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/run/user/1000/gvfs/smb-share:server=andlab_nas.local,share=andlab/ByeongSu/llama-ko.ipynb 셀 17\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/run/user/1000/gvfs/smb-share%3Aserver%3Dandlab_nas.local%2Cshare%3Dandlab/ByeongSu/llama-ko.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m코로나는 어떻게 전염돼?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/run/user/1000/gvfs/smb-share%3Aserver%3Dandlab_nas.local%2Cshare%3Dandlab/ByeongSu/llama-ko.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(task \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m\"\u001b[39m, model \u001b[39m=\u001b[39m model, tokenizer \u001b[39m=\u001b[39m tokenizer, max_length \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/run/user/1000/gvfs/smb-share%3Aserver%3Dandlab_nas.local%2Cshare%3Dandlab/ByeongSu/llama-ko.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m result \u001b[39m=\u001b[39m pipe(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m<s>[INST] \u001b[39;49m\u001b[39m{\u001b[39;49;00mprompt\u001b[39m}\u001b[39;49;00m\u001b[39m [/INST]\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/run/user/1000/gvfs/smb-share%3Aserver%3Dandlab_nas.local%2Cshare%3Dandlab/ByeongSu/llama-ko.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(result[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:208\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(text_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:271\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m prefix_length\n\u001b[1;32m    270\u001b[0m \u001b[39m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    272\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/transformers/generation/utils.py:1606\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[1;32m   1590\u001b[0m         input_ids,\n\u001b[1;32m   1591\u001b[0m         assistant_model\u001b[39m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1602\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1603\u001b[0m     )\n\u001b[1;32m   1604\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1605\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1606\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1607\u001b[0m         input_ids,\n\u001b[1;32m   1608\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1609\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1610\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1611\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1612\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1613\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1614\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1615\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1616\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1617\u001b[0m     )\n\u001b[1;32m   1619\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1620\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/transformers/generation/utils.py:2454\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2453\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2454\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2455\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2456\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2457\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2458\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2459\u001b[0m )\n\u001b[1;32m   2461\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2462\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:1038\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1035\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1037\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1039\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1040\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1041\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1042\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1043\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1044\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1045\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1046\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1047\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1048\u001b[0m )\n\u001b[1;32m   1050\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:925\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    921\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    922\u001b[0m         create_custom_forward(decoder_layer), hidden_states, attention_mask, position_ids\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    924\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 925\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    926\u001b[0m         hidden_states,\n\u001b[1;32m    927\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    928\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    929\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    930\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    931\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    932\u001b[0m         padding_mask\u001b[39m=\u001b[39;49mpadding_mask,\n\u001b[1;32m    933\u001b[0m     )\n\u001b[1;32m    935\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    937\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:635\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    632\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    634\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 635\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    636\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    637\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    638\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    639\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    640\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    641\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    642\u001b[0m     padding_mask\u001b[39m=\u001b[39;49mpadding_mask,\n\u001b[1;32m    643\u001b[0m )\n\u001b[1;32m    644\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    646\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:349\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    346\u001b[0m     value_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(value_states, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    348\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m     query_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_proj(hidden_states)\n\u001b[1;32m    350\u001b[0m     key_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    351\u001b[0m     value_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/peft/tuners/lora.py:908\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    902\u001b[0m     result \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlinear(x, transpose(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfan_in_fan_out), bias\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n\u001b[1;32m    904\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlora_A[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactive_adapter]\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    906\u001b[0m     result \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlora_B[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactive_adapter](\n\u001b[0;32m--> 908\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlora_A[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactive_adapter](\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlora_dropout[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactive_adapter](x))\n\u001b[1;32m    909\u001b[0m         )\n\u001b[1;32m    910\u001b[0m         \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaling[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactive_adapter]\n\u001b[1;32m    911\u001b[0m     )\n\u001b[1;32m    912\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    913\u001b[0m     result \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlinear(x, transpose(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfan_in_fan_out), bias\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytensor/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = \"코로나는 어떻게 전염돼?\"\n",
    "pipe = pipeline(task = \"text-generation\", model = model, tokenizer = tokenizer, max_length = 200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c0077f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] 코로나 감염 경로가 뭐야? [/INST]중국에서온사람과접촉한경우,중국에서온사람과접촉한경우,중국에서온사람과접촉한경우,중국에서\n",
      "[{'generated_text': '<s>[INST] 코로나 감염 경로가 뭐야? [/INST]중국에서온사람과접촉한경우,중국에서온사람과접촉한경우,중국에서온사람과접촉한경우,중국에서'}]\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = \"코로나 감염 경로가 뭐야?\"\n",
    "pipe = pipeline(task = \"text-generation\", model = model, tokenizer = tokenizer, max_length = 50)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "# s = resultsplit('a')\n",
    "print(result[0]['generated_text'])\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
